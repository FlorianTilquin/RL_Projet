\documentclass[a4paper,10pt]{article}
\usepackage{dsfont}
\usepackage{bbold}
\usepackage{stmaryrd}
\usepackage{mathrsfs}
\usepackage{fancyhdr}
\usepackage{algorithm,algpseudocode}
\usepackage{graphicx}
\input{zMC.tex}
\input{zMCgraphe.tex}
\zzpackages[english]

\tikzset{
zplot/.style={opacity=.8}
}

\begin{document}

\title{MVA Reinforcement Learning\\
Optimization of very difficult functions}

\author{Nathan de Lara, Florian Tilquin}
%\date{}
\maketitle

\section{Introduction}
\subsection{Problem statement}
The goal of this paper is to test and compare recently developed algorithms for the optimization of \textit{very difficult functions}. This work is based on the papers by Bubeck~\cite{bubeck2011x}, Grill~\cite{grill2015black}, Lazaric~\cite{azar2014online}, Bull~\cite{bull2013adaptive} and Valko~\cite{valko2013stochastic}. Each one of this paper has a specific definition of \textit{difficult} but the general idea is that the function to optimize has many local maxima and only one global maximum, it has very fast variations and is not necessarily differentiable such that a gradient-based approach to find the optimum should not be successful. All functions are assumed to be bounded and to have a compact support which, up to scaling can be fixed to be $[0,1]^p$. In this paper, we only consider $p=1$. In the end, the general formulation of the problem is:
\begin{equation}
\mbox{maximize } f(x) \mbox{ for } x\in [0,1]
\end{equation}

\subsection{About the multi-armed bandit}
\label{mab}
As previously mentioned, a gradient-based approach is not likely to perform well on the considered functions. Thus, the idea is to use a multi-armed bandit with theoretically an infinite number of arms, sometimes called \textit{continuum-armed bandit}. The algorithms progressively defines a sequence of evaluation points $x_t$ and observes a reward $y_t=f(x_t)+\xi_t$ where $\xi_t$ is a noise term. The choice of $x_{t+1}$ depends on the sequence of $(y_{t'})_{t'\le t}$. Depending on the algorithm, it is meant to converge to $x^*$ while controlling the cumulative regret:
\begin{equation}
\label{regret}
R_T = \underset{t=1}{\overset{T}{\sum}}f(x^*)-f(x_t)
\end{equation}
Or just simply ensuring that a good estimate of $x^*$ is present in the sequence (i.e minimizing simple regret). They can also be designed to be robust to noise in function estimations or correlated feedback.
In practice, for the algorithms considered, $x_t = \mathcal{U}(I_t)$. This means that $x_t$ is pulled uniformly at random in an interval $I_t$ that is chosen by the algorithm. The set of intervals defines a tree structure such that the set of leaves is a partition of $[0,1]$. In the end, the original problem is transformed into a particular case of regular multi-armed bandit. Without any prior information on the function to optimize, we only consider here the family of \textit{dyadic trees}. As mentioned in~\cite{bull2013adaptive}, the dyadic tree on [0, 1) is the tree with root node [0, 1), and where each node [a, b) has children $[a, \frac{1}{2}(a+b), [ \frac{1}{2}(a+b), b)$. An example of dyadic tree is displayed in~\ref{dydtree}. 

\begin{figure}
\label{dydtree}
\centering
\includegraphics[scale=0.3]{dyadic}
\caption{A dyadic tree from~\cite{bull2013adaptive}}
\end{figure}

\subsection{Notations}
In the rest of this paper, we note:
\begin{itemize}
\item $h,i$ the coordinates of a node in the dyadic tree such that $h$ is the depth of the node and $i$ its width
\item $\widehat{\mu}_{h,i}$ the empirical estimate of the reward associated to the node $h,i$
\item $n_{h,i}$ the number of times the node $h,i$ has been hit during the evaluations up to the current time
\item $T_{max}$ the total number of evaluations of the function allowed or \textit{budget}
\end{itemize} 



\section{Algorithms}
\label{algo}
In this section, we list the algorithms to be compared and briefly present their respective behaviors. The reader is welcome to consult the original papers for more detailed descriptions and proofs on theoretical performances. As previously mentioned, each algorithm defines a sequence of intervals $I_t$ from which $x_t$ is sampled. Typically, the selected interval maximizes a certain selection function of $(x_{t'},y_{t'})_{t'< t}$ among a subset of considered intervals. The main differences in the following algorithms is the definition of the selection function, which is most of the time an upper bound on the expected reward of the arm and the choice of the considered intervals at each time step, which is sometimes called "expansion rule".

\subsection{Hierarchical and Parallel Optimistic Optimization}
This algorithm is called \textit{Optimistic} because its idea is to sequentially building a tree and try the most promising children.
\paragraph{Upper bounds} $B_{h,i}=min(U_{h,i},max(B_{h+1,2i-1},B_{h,2i}))$ where:
\begin{equation}
\label{uhoo}
U_{h,i}=\widehat{\mu}_{h,i}+\nu \rho^h+\sqrt{\dfrac{2\log(T_{max})}{n_{h,i}}}
\end{equation}
\paragraph{Update rule} At each time step, the most promising child with respect to B is added to the tree.

\paragraph{Optimistic Optimization}
When there is no prior knowledge on the function to optimize, the tuning of the parameters $\nu$ and $\rho$ of equation~\ref{uhoo} can be difficult. Thus, the idea of this algorithm is to sequentially launch several HOOs that keep running in parallel in order to get the best parameters. Note that this algorithm can be run with other algorithms than HOO such as HCT.

\subsection{High-Confidence Tree}
This algorithm exists in two different versions: with correlated and uncorrelated feedback. For the purpose of performance comparison with other algorithms, we only consider the uncorrelated feedback, which is close to HOO algorithm. In order to be pulled, an arm must maximize $B$ among the arms that have not been pulled enough yet. In order to reduce computational cost, $U$ is refreshed only when $t$ is a power of 2.
\paragraph{Upper bounds} $B_{h,i}=min(U_{h,i},max(B_{h+1,2i-1},B_{h,2i}))$ where:
\begin{equation}
\label{uhct}
U_{h,i}=\widehat{\mu}_{h,i}+\nu \rho^h+\sqrt{\dfrac{c^2\log(1/min(1,\frac{c_1\delta}{2^{\lfloor \log(t) \rfloor + 1}}))}{n_{h,i}}}
\end{equation}
\paragraph{Update rule} Only the leaves of the current covering tree that have not been pulled enough with respect to a certain threshold $\tau_h(t)$ are expanded.

\subsection{Stochastic Simultaneous Optimistic Optimization}
This algorithm is called \textit{simultaneous} because it can perform at each time steps as many evaluations as the depth of its current covering tree. Indeed, the idea is that, in order not to rush into a local maximum, it is good to keep sampling from small depths until the estimation of the local reward is better known. Each node a its own evaluation budget $k$ in order no to spend to much budget on the first nodes.
\paragraph{Upper bounds} In order to be pulled, a leaf must both have a positive remaining individual budget and maximize $B$ among the leaves of the same depth:
\begin{equation}
\label{bsoo}
B_{h,i}=\widehat{\mu}_{h,i}+\sqrt{\dfrac{\log(\frac{T_{max}k}{\delta})}{2n_{h,i}}}
\end{equation}
\paragraph{Update rule} Once a node that maximizes $B$ has used its entire budget, it is expanded.


\subsection{Adaptive-Treed Bandits}
This algorithm has a slightly different spirit than the others. The tree is not sequentially built but given as an input and the evaluations are performed among a set of \textit{active boxes} or \textit{active nodes} (which in dimension 1 are simply intervals). Knowing the entire tree allows to update the statistics of the children of active nodes each time an arm is pulled, while in the other algorithms, only the statistics of the parents could be updated.
\paragraph{Upper bounds} At each time step, the arm pulled is chosen among the active nodes and maximizes:
\begin{equation}
B_{h,i} = \widehat{\mu}_{h,i}+(1+2\nu)r_{h,i}
\end{equation}
where $r_{h,i}=2\sqrt{\dfrac{\log[2^{h+1}(\tau+n_{h,i})]}{n_{h,i}}}$ is the confidence radius of the interval.
\paragraph{Update rule} If an active node has a radius small enough compared to the ones of its children, it is removed and replaced by them.

\section{Results}
\subsection{Experimental Setup}
\paragraph{Objective functions}
We test the algorithms on different reference functions from~\cite{valko2013stochastic} and~\cite{grill2015black}:
\begin{enumerate}
\item Two-sine product function: $f_1(x) = \frac{1}{2} (\sin(13x) . \sin(27x))+0.5$.
\item Garland function: $f_2(x) = 4x(1-x).(\frac{3}{4}+\frac{1}{4}(1-\sqrt{|\sin(60x)|}))$.
\item Grill function: $f_3(x) =1+ s(\log_2(|x-0.5|).(\sqrt{|x-0.5|}-(x-0.5)^2)-\sqrt{|x-0.5|}$ where $s(x)=\mathbf{1}(x- \lfloor x \rfloor \in [0,0.5])$.
\end{enumerate}
The associated plots are displayed in \ref{fig:functions}. It is important to have in mind that sampling a point at random for this functions would provide an average regret of approximately 0.46, 0.46 and 0.32 respectively.

\begin{figure}
\label{fig:functions}
\hbox{\hspace{-3.4cm}\begin{mygraph}{xmin=0, xmax=1, %
                ymin=0, ymax=1, %
                sizex=5, sizey=5}%
                {nomx=zbra, nomy=zbra}%
                {0,0.25,...,1}{0,0.25,...,1}
  \draw[zplot, blue] plot file {Data/Prodsin.data};
\end{mygraph}\hspace{-1cm}
\begin{mygraph}{xmin=0, xmax=1, %
                ymin=0, ymax=1, %
                sizex=5, sizey=5}%
                {nomx=zbra, nomy=zbra}%
                {0,0.25,...,1}{0,0.25,...,1}
  \draw[zplot, blue] plot file {Data/Garland.data};
\end{mygraph}\hspace{-1cm}
\begin{mygraph}{xmin=0, xmax=1, %
                ymin=-0.75, ymax=0.25, %
                sizex=5, sizey=5}%
                {nomx=zbra, nomy=zbra}%
                {0,0.25,...,1}{-0.75,-0.5,...,0.25}
  \draw[zplot, blue] plot file {Data/Grill.data};
\end{mygraph}\hss}
\caption{From the left to the right: Two-sine product, Garlang, Grill.}
\end{figure}

\paragraph{Algorithms setup}
In order to compare the performances of the different algorithms, we set a total number of function evaluations $T_{max}$ and a number of runs $N$. Then we compute for each and each algorithm run the best value returned $\widehat{x}^*$. Besides, we keep track of all the points sampled during each run and their associated reward in order to check the concentration of evaluations around the maximums and to compute quantities of interest such as the cumulative regret defined in~\ref{regret}, the average cumulative regret or the best estimate up to time $t$. The tuning of the parameters specific to each algorithm is performed manually. The algorithms are tested without noise and with different Gaussian noises.\\
As mentioned in section~\ref{mab}, the algorithms have different objectives (minimizing cumulative or simple regret for example), thus one must evaluate the performances according to each design.



\subsection{Analysis}

%%%%%%%%% ANNEXES %%%%%%%
\appendix
\section{S'more random shit in the appendix}
\subsection{HOO}
\begin{algorithm}[!Htop]
	\small
\caption{the hierarchical optimistic optimization algorithm}
\begin{algorithmic}[1]
	\For{n = 1 .. Nev}
		\State $P \gets {[0,1]}$
		\State $h,i\gets 0,1$
		\While{$[h,i] \in \mathcal{T}$}
			\If{$B_{h+1,2i-1} >B_{h+1,2i}$}
				\State $h,i \gets h+1,2i-1 $
			\ElsIf{$B_{h+1,2i-1} <B_{h+1,2i}$}
				\State $h,i \gets h+1,2i $
			\Else
				\State $Z \sim Ber(0.5)$
				\State $h,i \gets h+1,2i-Z$
			\EndIf
			\State $P \gets P \cup {[h,i]}$
		\EndWhile
		\State $H,I \gets h,i$
		\State $X \sim \mathcal{U}(\mathcal{P}_{H,I})$
		\State reward: $Y = f(X)$
		\State $\mathcal{T} \gets \mathcal{T} \cup {[H,I]}$
		\ForAll{$[h,i] \in P$}
			\State $T_{h,i} \gets T_{h,i}+1$
			\begin{large}
			\State $\mu_{h,i}\gets (1-\frac{1}{T_{h,i}})\mu_{h,i}+\frac{Y}{T_{h,i}}$
			\end{large}
		\EndFor
		\ForAll{$[h,i] \in \mathcal{T}$}
			\State $U_{h,i}\gets \mu_{h,i}+\sqrt{\frac{2\ln n}{T_{h,i}}}+\mu_1 \rho^h$
		\EndFor
		\State $B_{H+1,2I-1},B_{H+1,2I} \gets +\infty,+\infty$
		\State $\mathcal{T}' \gets \mathcal{T}$
		\While{$\mathcal{T}' \neq \{[0,1]\}$}
			\State $[h,i] \gets \text{Leaf}(\mathcal{T}')$
			\State $B_{h,i} \gets \min\{U_{h,i},B_{h+1,2i-1},B_{h+1,2i}\}$
			\State $\mathcal{T}' \gets \mathcal{T}'\{[h,i]\}$
		\EndWhile
	\EndFor
\end{algorithmic}
\end{algorithm}
\subsection{POO}
\begin{algorithm}[!Htop]
\caption{the hierarchical optimistic optimization algorithm}
\begin{algorithmic}[1]
	\For{n = 1 .. Nev}
		\State $P \gets {[0,1]}$
		\State $h,i\gets 0,1$
		\While{$[h,i] \in \mathcal{T}$}
			\If{$B_{h+1,2i-1} >B_{h+1,2i}$}
				\State $h,i \gets h+1,2i-1 $
			\ElsIf{$B_{h+1,2i-1} <B_{h+1,2i}$}
				\State $h,i \gets h+1,2i $
			\Else
				\State $Z \sim Ber(0.5)$
				\State $h,i \gets h+1,2i-Z$
			\EndIf
			\State $P \gets P \cup {[h,i]}$
		\EndWhile
		\State $H,I \gets h,i$
		\State $X \sim \mathcal{U}(\mathcal{P}_{H,I})$
		\State reward: $Y = f(X)$
		\State $\mathcal{T} \gets \mathcal{T} \cup {[H,I]}$
		\ForAll{$[h,i] \in P$}
			\State $T_{h,i} \gets T_{h,i}+1$
			\begin{large}
			\State $\mu_{h,i}\gets (1-\frac{1}{T_{h,i}})\mu_{h,i}+\frac{Y}{T_{h,i}}$
			\end{large}
		\EndFor
		\ForAll{$[h,i] \in \mathcal{T}$}
			\State $U_{h,i}\gets \mu_{h,i}+\sqrt{\frac{2\ln n}{T_{h,i}}}+\mu_1 \rho^h$
		\EndFor
		\State $B_{H+1,2I-1},B_{H+1,2I} \gets +\infty,+\infty$
		\State $\mathcal{T}' \gets \mathcal{T}$
		\While{$\mathcal{T}' \neq \{[0,1]\}$}
			\State $[h,i] \gets \text{Leaf}(\mathcal{T}')$
			\State $B_{h,i} \gets \min\{U_{h,i},B_{h+1,2i-1},B_{h+1,2i}\}$
			\State $\mathcal{T}' \gets \mathcal{T}'\{[h,i]\}$
		\EndWhile
	\EndFor
\end{algorithmic}
\end{algorithm}
\subsection{SOO}
\begin{algorithm}[!ht]
\caption{the hierarchical optimistic optimization algorithm}
\begin{algorithmic}[1]
	\For{n = 1 .. Nev}
		\State $P \gets {[0,1]}$
		\State $h,i\gets 0,1$
		\While{$[h,i] \in \mathcal{T}$}
			\If{$B_{h+1,2i-1} >B_{h+1,2i}$}
				\State $h,i \gets h+1,2i-1 $
			\ElsIf{$B_{h+1,2i-1} <B_{h+1,2i}$}
				\State $h,i \gets h+1,2i $
			\Else
				\State $Z \sim Ber(0.5)$
				\State $h,i \gets h+1,2i-Z$
			\EndIf
			\State $P \gets P \cup {[h,i]}$
		\EndWhile
		\State $H,I \gets h,i$
		\State $X \sim \mathcal{U}(\mathcal{P}_{H,I})$
		\State reward: $Y = f(X)$
		\State $\mathcal{T} \gets \mathcal{T} \cup {[H,I]}$
		\ForAll{$[h,i] \in P$}
			\State $T_{h,i} \gets T_{h,i}+1$
			\begin{large}
			\State $\mu_{h,i}\gets (1-\frac{1}{T_{h,i}})\mu_{h,i}+\frac{Y}{T_{h,i}}$
			\end{large}
		\EndFor
		\ForAll{$[h,i] \in \mathcal{T}$}
			\State $U_{h,i}\gets \mu_{h,i}+\sqrt{\frac{2\ln n}{T_{h,i}}}+\mu_1 \rho^h$
		\EndFor
		\State $B_{H+1,2I-1},B_{H+1,2I} \gets +\infty,+\infty$
		\State $\mathcal{T}' \gets \mathcal{T}$
		\While{$\mathcal{T}' \neq \{[0,1]\}$}
			\State $[h,i] \gets \text{Leaf}(\mathcal{T}')$
			\State $B_{h,i} \gets \min\{U_{h,i},B_{h+1,2i-1},B_{h+1,2i}\}$
			\State $\mathcal{T}' \gets \mathcal{T}'\{[h,i]\}$
		\EndWhile
	\EndFor
\end{algorithmic}
\end{algorithm}
\subsection{HCT}
\begin{algorithm}[!ht]
\caption{the hierarchical optimistic optimization algorithm}
\begin{algorithmic}[1]
	\For{n = 1 .. Nev}
		\State $P \gets {[0,1]}$
		\State $h,i\gets 0,1$
		\While{$[h,i] \in \mathcal{T}$}
			\If{$B_{h+1,2i-1} >B_{h+1,2i}$}
				\State $h,i \gets h+1,2i-1 $
			\ElsIf{$B_{h+1,2i-1} <B_{h+1,2i}$}
				\State $h,i \gets h+1,2i $
			\Else
				\State $Z \sim Ber(0.5)$
				\State $h,i \gets h+1,2i-Z$
			\EndIf
			\State $P \gets P \cup {[h,i]}$
		\EndWhile
		\State $H,I \gets h,i$
		\State $X \sim \mathcal{U}(\mathcal{P}_{H,I})$
		\State reward: $Y = f(X)$
		\State $\mathcal{T} \gets \mathcal{T} \cup {[H,I]}$
		\ForAll{$[h,i] \in P$}
			\State $T_{h,i} \gets T_{h,i}+1$
			\begin{large}
			\State $\mu_{h,i}\gets (1-\frac{1}{T_{h,i}})\mu_{h,i}+\frac{Y}{T_{h,i}}$
			\end{large}
		\EndFor
		\ForAll{$[h,i] \in \mathcal{T}$}
			\State $U_{h,i}\gets \mu_{h,i}+\sqrt{\frac{2\ln n}{T_{h,i}}}+\mu_1 \rho^h$
		\EndFor
		\State $B_{H+1,2I-1},B_{H+1,2I} \gets +\infty,+\infty$
		\State $\mathcal{T}' \gets \mathcal{T}$
		\While{$\mathcal{T}' \neq \{[0,1]\}$}
			\State $[h,i] \gets \text{Leaf}(\mathcal{T}')$
			\State $B_{h,i} \gets \min\{U_{h,i},B_{h+1,2i-1},B_{h+1,2i}\}$
			\State $\mathcal{T}' \gets \mathcal{T}'\{[h,i]\}$
		\EndWhile
	\EndFor
\end{algorithmic}
\end{algorithm}
\subsection{ATB}

%%% BIBILIOGRAPHY %%%%
\bibliographystyle{plain}
\bibliography{Biblio}{}
\nocite{*}

\label{lastpage}

\end{document}
